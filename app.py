from __future__ import annotations

import pandas as pd
import streamlit as st

from infralens.commands import ExecutionConfig, build_execution_templates
from infralens.data import Workload, default_workloads, sample_scenarios, workloads_for_scenario
from infralens.i18n import localize_findings, localize_recommendation, localize_severity
from infralens.llm import (
    generate_bottleneck_analysis,
    generate_recommendation_narrative,
    list_provider_models,
)
from infralens.parsers import parse_uploaded_telemetry
from infralens.report import build_pdf_report
from infralens.rules import build_placement_recommendation, detect_bottlenecks
from infralens.scoring import calculate_efficiency_score, infer_workload_profile
from infralens.metrics import collect_success_metrics, load_recent_metrics
from infralens.validation import validate_execution_settings


I18N = {
    "ko": {
        "title": "InfraLens - GPU Ïù∏ÌîÑÎùº ÏµúÏ†ÅÌôî AI ÏóêÏù¥Ï†ÑÌä∏",
        "caption": "MVP Îç∞Î™®: Ìö®Ïú® Ï†êÏàò + Î≥ëÎ™©/ÎπÑÌö®Ïú® Î∂ÑÏÑù + ÏµúÏ†Å Î∞∞Ïπò",
        "lang": "Ïñ∏Ïñ¥",
        "source": "ÌÖîÎ†àÎ©îÌä∏Î¶¨ ÏÜåÏä§",
        "source_help": "ÌÖîÎ†àÎ©îÌä∏Î¶¨Îäî ÏÑúÎ≤Ñ ÏÉÅÌÉúÎ•º ÏãúÍ∞Ñ ÏàúÏÑúÎ°ú Î™®ÏùÄ Ï∏°Ï†ï Í∏∞Î°ùÏûÖÎãàÎã§.",
        "sample": "ÏÉòÌîå Îç∞Ïù¥ÌÑ∞",
        "upload": "nvidia-smi ÏóÖÎ°úÎìú",
        "scenario": "ÏãúÎÇòÎ¶¨Ïò§",
        "scenario_help": "ÏãúÎÇòÎ¶¨Ïò§Îäî Î∂ÑÏÑù ÎåÄÏÉÅ ÏÑúÎ≤Ñ ÏÉÅÌÉú 1ÏÑ∏Ìä∏ÏûÖÎãàÎã§. (GPU Ïàò, ÏÇ¨Ïö©Î•†, Î©îÎ™®Î¶¨, NUMA/NVLink Ï†ïÎ≥¥)",
        "sample_scenario_help": "ÏÉòÌîå ÏãúÎÇòÎ¶¨Ïò§Îäî Îç∞Î™®/ÌÖåÏä§Ìä∏Ïö©ÏúºÎ°ú ÎØ∏Î¶¨ Ï§ÄÎπÑÎêú ÏÑúÎ≤Ñ ÏÉÅÌÉú Îç∞Ïù¥ÌÑ∞ÏûÖÎãàÎã§.",
        "upload_label": "ÌÖîÎ†àÎ©îÌä∏Î¶¨ ÌååÏùº ÏóÖÎ°úÎìú (.csv)",
        "upload_topo_label": "ÏÑ†ÌÉù: nvidia-smi topo ÌååÏùº ÏóÖÎ°úÎìú (.txt)",
        "upload_numa_label": "ÏÑ†ÌÉù: numactl --hardware ÌååÏùº ÏóÖÎ°úÎìú (.txt)",
        "upload_info": "Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ ÌÖîÎ†àÎ©îÌä∏Î¶¨ ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî.",
        "upload_ok": "GPU {n}Í∞úÎ•º {name}ÏóêÏÑú Î°úÎìúÌñàÏäµÎãàÎã§.",
        "upload_topo_ok": "ÌÜ†Ìè¥Î°úÏßÄ Ïò§Î≤ÑÎùºÏù¥Îìú Ï†ÅÏö©Îê® (topo: {topo}, numactl: {numa})",
        "upload_fail": "ÏóÖÎ°úÎìú ÌååÏã± Ïã§Ìå®: {err}",
        "workload": "ÏûëÏóÖ ÏÑ§Ï†ï",
        "workload_help": "ÏûëÏóÖ(ÏõåÌÅ¨Î°úÎìú)ÏùÄ ÏÑúÎ≤ÑÏóêÏÑú Ïã§ÌñâÌï† ÌîÑÎ°úÍ∑∏Îû® Îã®ÏúÑÏûÖÎãàÎã§. Ïòà: ÌïôÏäµ ÏûëÏóÖ, Ï∂îÎ°† API.",
        "telemetry_word_help": "ÌÖîÎ†àÎ©îÌä∏Î¶¨(telemetry): GPU ÏÇ¨Ïö©Î•†/Î©îÎ™®Î¶¨ Í∞ôÏùÄ Ïö¥ÏòÅ Ï∏°Ï†ï Îç∞Ïù¥ÌÑ∞",
        "analyze": "Î∂ÑÏÑù ÏãúÏûë",
        "score": "Ìö®Ïú® Ï†êÏàò",
        "grade": "Îì±Í∏â",
        "gpu_component": "GPU Íµ¨ÏÑ± Ï†êÏàò",
        "numa_component": "NUMA Íµ¨ÏÑ± Ï†êÏàò",
        "telemetry": "ÌÖîÎ†àÎ©îÌä∏Î¶¨ Ïä§ÎÉÖÏÉ∑",
        "telemetry_help_ui": "ÌòÑÏû¨ ÏãúÎÇòÎ¶¨Ïò§Ïùò GPU ÏÉÅÌÉú ÏûÖÎ†•Í∞í ÌëúÏûÖÎãàÎã§.",
        "findings": "Î≥ëÎ™©/ÎπÑÌö®Ïú® Î∂ÑÏÑù Í≤∞Í≥º",
        "findings_help": "Î£∞ ÏóîÏßÑÏù¥ Í∞êÏßÄÌïú Î≥ëÎ™©/ÎπÑÌö®Ïú® Ïù¥Î≤§Ìä∏ Î™©Î°ùÏûÖÎãàÎã§.",
        "analysis": "AI Î≥ëÎ™©/ÎπÑÌö®Ïú® Î∂ÑÏÑù",
        "analysis_help_ui": "LLM ÎòêÎäî fallbackÏúºÎ°ú ÏÉùÏÑ±Îêú ÏõêÏù∏ ÏÑ§Î™ÖÏûÖÎãàÎã§.",
        "placement": "ÏµúÏ†Å Î∞∞Ïπò Ï†úÏïà",
        "placement_help": "Ï∂îÏ≤úÏùÄ ÏûëÏóÖÏùÑ Ïñ¥Îñ§ GPU/NUMAÏóê Î∞∞ÏπòÌïòÎ©¥ Îçî Ìö®Ïú®Ï†ÅÏù∏ÏßÄ Ï†úÏãúÌï©ÎãàÎã§.",
        "expected": "ÏòàÏÉÅ Í∞úÏÑ†: ÌôúÏö©Î•† {before}% -> {after}%, ÌïôÏäµ +{train}%, Ï∂îÎ°† Î†àÏù¥ÌÑ¥Ïãú -{lat}%",
        "narrative": "Ï∂îÏ≤ú ÏÑ§Î™Ö",
        "narrative_help": "Ï∂îÏ≤ú Î∞∞ÏπòÎ•º Ïôú Ï†úÏïàÌñàÎäîÏßÄ ÏûêÏó∞Ïñ¥Î°ú ÏÑ§Î™ÖÌï©ÎãàÎã§.",
        "exec_title": "Ïã§Ìñâ Í∞ÄÎä•Ìïú Î™ÖÎ†πÏñ¥ ÌÖúÌîåÎ¶ø",
        "exec_desc": "Ï∂îÏ≤ú Í≤∞Í≥º Í∏∞Î∞ò ÌÖúÌîåÎ¶øÏûÖÎãàÎã§. ÌôòÍ≤ΩÏóê ÎßûÍ≤å Ïù¥ÎØ∏ÏßÄ/Ïä§ÌÅ¨Î¶ΩÌä∏ Í≤ΩÎ°úÎ•º ÏàòÏ†ïÌïòÏÑ∏Ïöî.",
        "exec_numactl": "numactl",
        "exec_taskset": "taskset",
        "exec_docker": "docker",
        "pdf": "PDF Î¶¨Ìè¨Ìä∏ Îã§Ïö¥Î°úÎìú",
        "idle": "ÏãúÎÇòÎ¶¨Ïò§Î•º ÏÑ†ÌÉùÌïòÍ≥† Î∂ÑÏÑù ÏãúÏûëÏùÑ ÎàåÎü¨ ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ïã§ÌñâÌïòÏÑ∏Ïöî.",
        "source_caption": "ÏÉùÏÑ± ÏÜåÏä§: {source}",
        "profile": "ÌîÑÎ°úÌïÑ",
        "profile_default": "Í∏∞Î≥∏",
        "profile_training": "ÌïôÏäµ Ï§ëÏã¨",
        "profile_inference": "Ï∂îÎ°† Ï§ëÏã¨",
        "cmd_title": "Ïã§Ìñâ Î™ÖÎ†π (Î≥µÏÇ¨Ìï¥ÏÑú ÏÇ¨Ïö©)",
        "cmd_gpu": "1) GPU ÌÖîÎ†àÎ©îÌä∏Î¶¨ CSV",
        "cmd_topo": "2) GPU ÌÜ†Ìè¥Î°úÏßÄ ÌÖçÏä§Ìä∏",
        "cmd_numa": "3) NUMA ÌÜ†Ìè¥Î°úÏßÄ ÌÖçÏä§Ìä∏",
        "llm_title": "LLM API ÏÑ§Ï†ï",
        "llm_enable": "UIÏóêÏÑú LLM API ÏÇ¨Ïö©",
        "llm_provider": "Provider",
        "llm_api_key": "API Key ({provider})",
        "llm_model": "Model",
        "llm_help": "API KeyÎ•º ÏûÖÎ†•ÌïòÏßÄ ÏïäÏúºÎ©¥ Î£∞ Í∏∞Î∞ò fallbackÏúºÎ°ú ÎèôÏûëÌï©ÎãàÎã§.",
        "llm_models_loaded": "{provider} Î™®Îç∏ Î™©Î°ùÏùÑ Î∂àÎü¨ÏôîÏäµÎãàÎã§.",
        "llm_models_fallback": "Î™®Îç∏ Î™©Î°ù Ï°∞ÌöåÏóê Ïã§Ìå®Ìï¥ Í∏∞Î≥∏ Ï∂îÏ≤ú Î™®Îç∏ÏùÑ ÌëúÏãúÌï©ÎãàÎã§.",
        "llm_working": "LLM Î∂ÑÏÑù/Ï∂îÏ≤ú ÏÉùÏÑ± Ï§ëÏûÖÎãàÎã§...",
        "llm_working_detail": "Ïû†ÏãúÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî. Î≥ëÎ™©/ÎπÑÌö®Ïú® Î∂ÑÏÑùÍ≥º Î∞∞Ïπò ÏÑ§Î™ÖÏùÑ ÏÉùÏÑ±ÌïòÍ≥† ÏûàÏäµÎãàÎã§.",
        "metrics_title": "ÏÑ±Í≥µÏßÄÌëú Ï∏°Ï†ï",
        "metrics_help": "Ïù¥ ÏÑπÏÖòÏùÄ ÏµúÏ†ÅÌôî Ï†Ñ/ÌõÑÎ•º Í∞ôÏùÄ Í∏∞Ï§ÄÏúºÎ°ú Ï∏°Ï†ïÌï¥ ÎπÑÍµêÌï©ÎãàÎã§.",
        "metrics_clarity_title": "Ï∏°Ï†ï Í∏∞Ï§Ä ÏöîÏïΩ",
        "metrics_clarity_1": "ÏãúÎÇòÎ¶¨Ïò§: Î∂ÑÏÑù ÎåÄÏÉÅ ÏÑúÎ≤Ñ ÏÉÅÌÉú 1ÏÑ∏Ìä∏ (GPU Ïàò/ÏÇ¨Ïö©Î•†/Î©îÎ™®Î¶¨/NUMA/NVLink).",
        "metrics_clarity_2": "ÏÉòÌîå ÏãúÎÇòÎ¶¨Ïò§: ÏúÑ ÏÑúÎ≤Ñ ÏÉÅÌÉúÎ•º ÎØ∏Î¶¨ Ï§ÄÎπÑÌïú Îç∞Î™®/ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞.",
        "metrics_clarity_3": "Ï∂îÏ≤ú: ÏûëÏóÖÏùÑ Ïñ¥Îñ§ GPU/NUMAÏóê Î∞∞ÏπòÌï†ÏßÄ Ï†úÏïàÌïòÎäî Í≤∞Í≥º.",
        "metrics_clarity_4": "Ï∂îÏ≤ú ÏùºÏπòÏú®: Î≥ëÎ™©Ïù¥ ÏûàÏúºÎ©¥ Ï∂îÏ≤úÏù¥ ÏûàÏñ¥Ïïº ÌïòÍ≥†, Î≥ëÎ™©Ïù¥ ÏóÜÏúºÎ©¥ Ï∂îÏ≤úÏù¥ ÏóÜÏñ¥Ïïº 'ÏùºÏπò'.",
        "metrics_clarity_5": "ÏòàÏãú: Î≥ëÎ™© ÏûàÏùå + Ï∂îÏ≤ú ÏûàÏùå = ÏùºÏπò, Î≥ëÎ™© ÏóÜÏùå + Ï∂îÏ≤ú ÏóÜÏùå = ÏùºÏπò.",
        "metrics_phase": "Ï∏°Ï†ï Îã®Í≥Ñ",
        "metrics_phase_before": "ÏµúÏ†ÅÌôî Ï†Ñ",
        "metrics_phase_after": "ÏµúÏ†ÅÌôî ÌõÑ",
        "metrics_iterations": "Î∞òÎ≥µ ÌöüÏàò",
        "metrics_run": "ÏÑ±Í≥µÏßÄÌëú Ï∏°Ï†ï Ïã§Ìñâ",
        "metrics_recent": "ÏµúÍ∑º Ï∏°Ï†ï Î°úÍ∑∏",
        "metrics_log_path": "Î°úÍ∑∏ ÌååÏùº",
        "metrics_compare": "Ï†Ñ/ÌõÑ ÎπÑÍµê ÏöîÏïΩ",
        "metrics_no_compare": "Ï†Ñ/ÌõÑ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏïÑÏßÅ Î∂ÄÏ°±Ìï©ÎãàÎã§.",
        "metrics_report_download": "Ï†Ñ/ÌõÑ ÎπÑÍµê Î¶¨Ìè¨Ìä∏ Îã§Ïö¥Î°úÎìú(.md)",
        "metrics_report_title": "ÏÑ±Í≥µÏßÄÌëú Ï†Ñ/ÌõÑ ÎπÑÍµê Î¶¨Ìè¨Ìä∏",
        "metrics_desc_response": "ÏùëÎãµÏãúÍ∞Ñ: ÎÇÆÏùÑÏàòÎ°ù Ï¢ãÏäµÎãàÎã§. scoreÎäî Ï†êÏàò Í≥ÑÏÇ∞ ÏãúÍ∞Ñ, analysis_pipelineÏùÄ Î≥ëÎ™©ÌÉêÏßÄ+Ï∂îÏ≤ú ÏÉùÏÑ± ÏãúÍ∞ÑÏûÖÎãàÎã§.",
        "metrics_desc_test": "ÌÖåÏä§Ìä∏ ÌÜµÍ≥ºÏú®: ÎÜíÏùÑÏàòÎ°ù Ï¢ãÏäµÎãàÎã§. `python -m unittest discover -s tests -p 'test_*.py'` Í≤∞Í≥ºÎ°ú Í≥ÑÏÇ∞Ìï©ÎãàÎã§.",
        "metrics_desc_test_formula": "Í≥ÑÏÇ∞Ïãù: passed_tests / total_tests",
        "metrics_desc_reco": "Ï∂îÏ≤ú ÏùºÏπòÏú®: ÎÜíÏùÑÏàòÎ°ù Ï¢ãÏäµÎãàÎã§. Î≥ëÎ™© Ïú†Î¨¥ÏôÄ Ï∂îÏ≤ú ÌïÑÏöî Ïó¨Î∂ÄÍ∞Ä ÏùºÏπòÌïòÎäî ÎπÑÏú®ÏûÖÎãàÎã§.",
        "metrics_desc_reco_formula": "ÏùºÏπò Í∏∞Ï§Ä: (Î≥ëÎ™© ÏûàÏùå & Ï∂îÏ≤ú ÏûàÏùå) ÎòêÎäî (Î≥ëÎ™© ÏóÜÏùå & Ï∂îÏ≤ú ÏóÜÏùå)",
        "metrics_desc_reco_count": "Í≥ÑÏÇ∞Ïãù: consistency_successes / attempted_scenarios",
        "metrics_desc_reco_detail": "ÏÑ∏Î∂Ä Í∏∞Ï§Ä: ÏãúÎèÑ=ÏÉòÌîå ÏãúÎÇòÎ¶¨Ïò§ Í∞úÏàò x Î∞òÎ≥µÌöüÏàò. Ïù¥ ÏßÄÌëúÎäî Ï∂îÏ≤ú ÌíàÏßàÏù¥ ÏïÑÎãàÎùº 'Ï∂îÏ≤ú ÌïÑÏöîÏÑ± ÌåêÎã®'Ïùò ÏùºÏπòÎèÑÎ•º Ï∏°Ï†ïÌï©ÎãàÎã§.",
        "metrics_term_scenario": "ÏãúÎÇòÎ¶¨Ïò§: Î∂ÑÏÑù ÎåÄÏÉÅ ÏÑúÎ≤Ñ ÏÉÅÌÉú 1ÏÑ∏Ìä∏ÏûÖÎãàÎã§. (GPU Ïàò, ÏÇ¨Ïö©Î•†, Î©îÎ™®Î¶¨, NUMA/NVLink Ï†ïÎ≥¥)",
        "metrics_term_sample_scenario": "ÏÉòÌîå ÏãúÎÇòÎ¶¨Ïò§: Îç∞Î™®/ÌÖåÏä§Ìä∏Ïö©ÏúºÎ°ú ÎØ∏Î¶¨ Ï§ÄÎπÑÎêú ÏÑúÎ≤Ñ ÏÉÅÌÉú Îç∞Ïù¥ÌÑ∞ÏûÖÎãàÎã§.",
        "metrics_term_recommendation": "Ï∂îÏ≤ú: Í∞Å ÏûëÏóÖÏùÑ Ïñ¥Îñ§ GPU/NUMAÏóê Î∞∞ÏπòÌïòÎ©¥ Îçî Ìö®Ïú®Ï†ÅÏù∏ÏßÄ Ï†úÏïàÌïòÎäî Í≤∞Í≥ºÏûÖÎãàÎã§. (Ïòà: NVLink Î°úÏª¨ Í∑∏Î£π Î∞∞Ïπò, MIG Î∂ÑÎ¶¨)",
        "exec_settings_title": "Ïã§Ìñâ ÏÑ§Ï†ï",
        "exec_show_host_ref": "Ìò∏Ïä§Ìä∏ Î™ÖÎ†πÎèÑ Ìï®Íªò Î≥¥Í∏∞(Ï∞∏Í≥†Ïö©)",
        "exec_host_ref_title": "Host reference commands",
        "exec_validation_prefix": "Ïã§Ìñâ ÏÑ§Ï†ï ÌôïÏù∏",
        "exec_validation_entry": "Ïã§Ìñâ Î™ÖÎ†πÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî.",
        "exec_validation_image": "Docker Î™®ÎìúÏóêÏÑúÎäî Docker Ïù¥ÎØ∏ÏßÄÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.",
        "exec_validation_cpu": "Manual CPU Set Î™®ÎìúÏóêÏÑúÎäî CPU Set Í∞íÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî.",
        "exec_validation_env": "Env Vars ÌòïÏãù Ïò§Î•ò ÎùºÏù∏: {lines}",
        "report_labels": {
            "scenario": "ÏãúÎÇòÎ¶¨Ïò§",
            "efficiency": "Ìö®Ïú® Ï†êÏàò",
            "component_scores": "Íµ¨ÏÑ± Ï†êÏàò",
            "gpu_component": "GPU Íµ¨ÏÑ± Ï†êÏàò",
            "numa_component": "NUMA Íµ¨ÏÑ± Ï†êÏàò",
            "network_component": "ÎÑ§Ìä∏ÏõåÌÅ¨ I/O Ï†êÏàò",
            "findings": "Î≥ëÎ™©/ÎπÑÌö®Ïú® Î∂ÑÏÑù Í≤∞Í≥º",
            "analysis": "AI Î≥ëÎ™©/ÎπÑÌö®Ïú® Î∂ÑÏÑù",
            "placement": "ÏµúÏ†Å Î∞∞Ïπò Ï†úÏïà",
            "expected": "ÏòàÏÉÅ Í∞úÏÑ†",
        },
    },
    "en": {
        "title": "InfraLens - GPU Infrastructure Optimization AI Agent",
        "caption": "MVP Demo: Resource Score + Bottleneck/Inefficiency Analysis + Optimal Placement",
        "lang": "Language",
        "source": "Telemetry Source",
        "source_help": "Telemetry means collected runtime metrics over time.",
        "sample": "Sample Data",
        "upload": "Upload nvidia-smi",
        "scenario": "Scenario",
        "scenario_help": "A scenario is one server-state snapshot to analyze (GPU count, utilization, memory, NUMA/NVLink).",
        "sample_scenario_help": "Sample scenarios are prebuilt demo/test server-state datasets.",
        "upload_label": "Upload telemetry file (.csv)",
        "upload_topo_label": "Optional: Upload nvidia-smi topology file (.txt)",
        "upload_numa_label": "Optional: Upload numactl --hardware file (.txt)",
        "upload_info": "Upload a telemetry file to run analysis.",
        "upload_ok": "Loaded {n} GPUs from {name}.",
        "upload_topo_ok": "Applied topology overrides (topo: {topo}, numactl: {numa})",
        "upload_fail": "Failed to parse upload: {err}",
        "workload": "Task Configuration",
        "workload_help": "A task(workload) is one runnable unit, such as model training or inference API.",
        "telemetry_word_help": "Telemetry: runtime metrics such as GPU utilization and memory usage.",
        "analyze": "Analyze",
        "score": "Efficiency Score",
        "grade": "Grade",
        "gpu_component": "GPU Component",
        "numa_component": "NUMA Component",
        "telemetry": "Telemetry Snapshot",
        "telemetry_help_ui": "Input GPU state table for the current scenario.",
        "findings": "Bottleneck/Inefficiency Findings",
        "findings_help": "List of bottlenecks/inefficiencies detected by the rule engine.",
        "analysis": "AI Bottleneck/Inefficiency Analysis",
        "analysis_help_ui": "Cause explanation generated by LLM or fallback.",
        "placement": "Optimal Placement Recommendation",
        "placement_help": "Recommendations suggest where to place each task for better efficiency.",
        "expected": "Expected improvement: Utilization {before}% -> {after}%, Training +{train}%, Inference Latency -{lat}%",
        "narrative": "Recommendation Narrative",
        "narrative_help": "Natural-language explanation of why the placement was recommended.",
        "exec_title": "Executable Command Templates",
        "exec_desc": "Templates are generated from recommendations. Adjust image/script paths to your environment.",
        "exec_numactl": "numactl",
        "exec_taskset": "taskset",
        "exec_docker": "docker",
        "pdf": "Download PDF Report",
        "idle": "Select a scenario and click Analyze to run the MVP pipeline.",
        "source_caption": "Source: {source}",
        "profile": "Profile",
        "profile_default": "Default",
        "profile_training": "Training-heavy",
        "profile_inference": "Inference-heavy",
        "cmd_title": "Collection Commands (Copy)",
        "cmd_gpu": "1) GPU telemetry CSV",
        "cmd_topo": "2) GPU topology text",
        "cmd_numa": "3) NUMA topology text",
        "llm_title": "LLM API Settings",
        "llm_enable": "Use LLM API from UI",
        "llm_provider": "Provider",
        "llm_api_key": "API Key ({provider})",
        "llm_model": "Model",
        "llm_help": "If API key is empty, rule-based fallback will be used.",
        "llm_models_loaded": "Loaded model list from {provider}.",
        "llm_models_fallback": "Could not fetch model list, showing recommended defaults.",
        "llm_working": "Generating LLM analysis and recommendations...",
        "llm_working_detail": "Please wait while bottleneck/inefficiency analysis and placement narratives are being prepared.",
        "metrics_title": "Success Metrics",
        "metrics_help": "This section measures before/after optimization with the same criteria.",
        "metrics_clarity_title": "How Metrics Are Measured",
        "metrics_clarity_1": "Scenario: one server-state snapshot (GPU count/utilization/memory/NUMA/NVLink).",
        "metrics_clarity_2": "Sample scenario: prebuilt demo/test server-state dataset.",
        "metrics_clarity_3": "Recommendation: suggested task placement across GPU/NUMA.",
        "metrics_clarity_4": "Recommendation consistency: if bottleneck exists, recommendation should exist; if no bottleneck, recommendation should be absent.",
        "metrics_clarity_5": "Example: bottleneck+recommendation = consistent, no-bottleneck+no-recommendation = consistent.",
        "metrics_phase": "Phase",
        "metrics_phase_before": "Before Optimization",
        "metrics_phase_after": "After Optimization",
        "metrics_iterations": "Iterations",
        "metrics_run": "Run Success Metrics",
        "metrics_recent": "Recent Metric Logs",
        "metrics_log_path": "Log file",
        "metrics_compare": "Before/After Summary",
        "metrics_no_compare": "Not enough before/after records yet.",
        "metrics_report_download": "Download Before/After Report (.md)",
        "metrics_report_title": "Success Metrics Before/After Report",
        "metrics_desc_response": "Response time: lower is better. score is score-calculation time, analysis_pipeline is bottleneck+recommendation generation time.",
        "metrics_desc_test": "Test pass rate: higher is better. It is measured by running `python -m unittest discover -s tests -p 'test_*.py'`.",
        "metrics_desc_test_formula": "Formula: passed_tests / total_tests",
        "metrics_desc_reco": "Recommendation consistency rate: higher is better. It measures whether recommendation need matches bottleneck presence.",
        "metrics_desc_reco_formula": "Consistent when: (bottleneck & recommendation) OR (no bottleneck & no recommendation)",
        "metrics_desc_reco_count": "Formula: consistency_successes / attempted_scenarios",
        "metrics_desc_reco_detail": "Detail: attempted scenarios = scenario_count x iterations. This is a need-consistency metric, not recommendation quality.",
        "metrics_term_scenario": "Scenario: one server-state snapshot to analyze (GPU count, utilization, memory, NUMA/NVLink).",
        "metrics_term_sample_scenario": "Sample scenario: prebuilt demo/test server-state dataset.",
        "metrics_term_recommendation": "Recommendation: suggested task-to-GPU/NUMA placement for better efficiency (e.g., NVLink-local placement, MIG split).",
        "exec_settings_title": "Execution Settings",
        "exec_show_host_ref": "Also show host commands (reference)",
        "exec_host_ref_title": "Host reference commands",
        "exec_validation_prefix": "Execution settings check",
        "exec_validation_entry": "Entry command is required.",
        "exec_validation_image": "Docker image is required in Docker mode.",
        "exec_validation_cpu": "Manual CPU set is required in Manual mode.",
        "exec_validation_env": "Invalid env var line(s): {lines}",
        "report_labels": {
            "scenario": "Scenario",
            "efficiency": "Efficiency Score",
            "component_scores": "Component Scores",
            "gpu_component": "GPU Component",
            "numa_component": "NUMA Component",
            "network_component": "Network I/O Component",
            "findings": "Bottleneck/Inefficiency Findings",
            "analysis": "AI Bottleneck/Inefficiency Analysis",
            "placement": "Optimal Placement Recommendation",
            "expected": "Expected Improvement",
        },
    },
    "zh": {
        "title": "InfraLens - GPU Âü∫Á°ÄËÆæÊñΩ‰ºòÂåñ AI Agent",
        "caption": "MVP ÊºîÁ§∫ÔºöÊïàÁéáËØÑÂàÜ + Áì∂È¢à/‰ΩéÊïàÂàÜÊûê + ÊúÄ‰ºòÈÉ®ÁΩ≤",
        "lang": "ËØ≠Ë®Ä",
        "source": "ÈÅ•ÊµãÊï∞ÊçÆÊù•Ê∫ê",
        "source_help": "ÈÅ•ÊµãÊòØÊåâÊó∂Èó¥ÈááÈõÜÁöÑËøêË°åÊåáÊ†áÊï∞ÊçÆ„ÄÇ",
        "sample": "Á§∫‰æãÊï∞ÊçÆ",
        "upload": "‰∏ä‰º† nvidia-smi",
        "scenario": "Âú∫ÊôØ",
        "scenario_help": "Âú∫ÊôØÊòØ‰∏ÄÊ¨°ÂæÖÂàÜÊûêÁöÑÊúçÂä°Âô®Áä∂ÊÄÅÂø´ÁÖßÔºàGPU Êï∞„ÄÅÂà©Áî®Áéá„ÄÅÊòæÂ≠ò„ÄÅNUMA/NVLinkÔºâ„ÄÇ",
        "sample_scenario_help": "Á§∫‰æãÂú∫ÊôØÊòØÈ¢ÑÁΩÆÁöÑÊºîÁ§∫/ÊµãËØïÊúçÂä°Âô®Áä∂ÊÄÅÊï∞ÊçÆ„ÄÇ",
        "upload_label": "‰∏ä‰º†ÈÅ•ÊµãÊñá‰ª∂Ôºà.csvÔºâ",
        "upload_topo_label": "ÂèØÈÄâÔºö‰∏ä‰º† nvidia-smi ÊãìÊâëÊñá‰ª∂Ôºà.txtÔºâ",
        "upload_numa_label": "ÂèØÈÄâÔºö‰∏ä‰º† numactl --hardware Êñá‰ª∂Ôºà.txtÔºâ",
        "upload_info": "ËØ∑‰∏ä‰º†ÈÅ•ÊµãÊñá‰ª∂ÂêéÂÜçÊâßË°åÂàÜÊûê„ÄÇ",
        "upload_ok": "Â∑≤‰ªé {name} Âä†ËΩΩ {n} Âº† GPU„ÄÇ",
        "upload_topo_ok": "Â∑≤Â∫îÁî®ÊãìÊâëË¶ÜÁõñÔºàtopo: {topo}, numactl: {numa}Ôºâ",
        "upload_fail": "Ëß£Êûê‰∏ä‰º†Êñá‰ª∂Â§±Ë¥•: {err}",
        "workload": "‰ªªÂä°ÈÖçÁΩÆ",
        "workload_help": "‰ªªÂä°(Â∑•‰ΩúË¥üËΩΩ)ÊòØ‰∏Ä‰∏™ÂèØÊâßË°åÂçïÂÖÉÔºå‰æãÂ¶ÇËÆ≠ÁªÉ‰ªªÂä°ÊàñÊé®ÁêÜ API„ÄÇ",
        "telemetry_word_help": "ÈÅ•ÊµãÔºöÂ¶Ç GPU Âà©Áî®Áéá„ÄÅÊòæÂ≠òÁ≠âËøêË°åÊåáÊ†á„ÄÇ",
        "analyze": "ÂºÄÂßãÂàÜÊûê",
        "score": "ÊïàÁéáËØÑÂàÜ",
        "grade": "Á≠âÁ∫ß",
        "gpu_component": "GPU ÁªÑ‰ª∂ÂàÜ",
        "numa_component": "NUMA ÁªÑ‰ª∂ÂàÜ",
        "telemetry": "ÈÅ•ÊµãÂø´ÁÖß",
        "telemetry_help_ui": "ÂΩìÂâçÂú∫ÊôØÁöÑ GPU Áä∂ÊÄÅËæìÂÖ•Ë°®„ÄÇ",
        "findings": "Áì∂È¢à/‰ΩéÊïàÂèëÁé∞",
        "findings_help": "ËßÑÂàôÂºïÊìéËØÜÂà´Âà∞ÁöÑÁì∂È¢à/‰ΩéÊïà‰∫ã‰ª∂ÂàóË°®„ÄÇ",
        "analysis": "AI Áì∂È¢à/‰ΩéÊïàÂàÜÊûê",
        "analysis_help_ui": "Áî± LLM Êàñ fallback ÁîüÊàêÁöÑÂéüÂõ†ËØ¥Êòé„ÄÇ",
        "placement": "ÊúÄ‰ºòÈÉ®ÁΩ≤Âª∫ËÆÆ",
        "placement_help": "Êé®Ëçê‰ºöÁªôÂá∫‰ªªÂä°Â∫îÊîæÂú®Âì™‰∫õ GPU/NUMA ‰∏äÊõ¥È´òÊïà„ÄÇ",
        "expected": "È¢ÑÊúüÊîπÂñÑÔºöÂà©Áî®Áéá {before}% -> {after}%ÔºåËÆ≠ÁªÉ +{train}%ÔºåÊé®ÁêÜÊó∂Âª∂ -{lat}%",
        "narrative": "Âª∫ËÆÆËØ¥Êòé",
        "narrative_help": "ÂØπÊé®ËçêÈÉ®ÁΩ≤ÊñπÊ°àÁªôÂá∫Ëá™ÁÑ∂ËØ≠Ë®ÄËß£Èáä„ÄÇ",
        "exec_title": "ÂèØÊâßË°åÂëΩ‰ª§Ê®°Êùø",
        "exec_desc": "Âü∫‰∫éÊé®ËçêÁªìÊûúÁîüÊàêÊ®°Êùø„ÄÇËØ∑ÊåâÁéØÂ¢É‰øÆÊîπÈïúÂÉèÂíåËÑöÊú¨Ë∑ØÂæÑ„ÄÇ",
        "exec_numactl": "numactl",
        "exec_taskset": "taskset",
        "exec_docker": "docker",
        "pdf": "‰∏ãËΩΩ PDF Êä•Âëä",
        "idle": "ËØ∑ÈÄâÊã©Âú∫ÊôØÂπ∂ÁÇπÂáªÂºÄÂßãÂàÜÊûê„ÄÇ",
        "source_caption": "Êù•Ê∫ê: {source}",
        "profile": "ÂàÜÊûêÊ°£‰Ωç",
        "profile_default": "ÈªòËÆ§",
        "profile_training": "ËÆ≠ÁªÉ‰ºòÂÖà",
        "profile_inference": "Êé®ÁêÜ‰ºòÂÖà",
        "cmd_title": "ÈááÈõÜÂëΩ‰ª§ÔºàÂèØÁõ¥Êé•Â§çÂà∂Ôºâ",
        "cmd_gpu": "1) GPU ÈÅ•Êµã CSV",
        "cmd_topo": "2) GPU ÊãìÊâëÊñáÊú¨",
        "cmd_numa": "3) NUMA ÊãìÊâëÊñáÊú¨",
        "llm_title": "LLM API ËÆæÁΩÆ",
        "llm_enable": "Âú® UI ‰∏≠‰ΩøÁî® LLM API",
        "llm_provider": "‰æõÂ∫îÂïÜ",
        "llm_api_key": "API KeyÔºà{provider}Ôºâ",
        "llm_model": "Ê®°Âûã",
        "llm_help": "Êú™Â°´ÂÜô API Key Êó∂Â∞Ü‰ΩøÁî®ËßÑÂàô fallback„ÄÇ",
        "llm_models_loaded": "Â∑≤‰ªé {provider} Âä†ËΩΩÊ®°ÂûãÂàóË°®„ÄÇ",
        "llm_models_fallback": "Ê®°ÂûãÂàóË°®Ëé∑ÂèñÂ§±Ë¥•ÔºåÂ∑≤ÊòæÁ§∫Êé®ËçêÈªòËÆ§Ê®°Âûã„ÄÇ",
        "llm_working": "Ê≠£Âú®ÁîüÊàê LLM ÂàÜÊûê‰∏éÂª∫ËÆÆ...",
        "llm_working_detail": "ËØ∑Á®çÂÄôÔºåÁ≥ªÁªüÊ≠£Âú®ÁîüÊàêÁì∂È¢à/‰ΩéÊïàÂàÜÊûêÂíåÈÉ®ÁΩ≤Âª∫ËÆÆËØ¥Êòé„ÄÇ",
        "metrics_title": "ÊàêÂäüÊåáÊ†áÊµãÈáè",
        "metrics_help": "Êú¨ËäÇ‰ΩøÁî®Âêå‰∏ÄÊ†áÂáÜÊµãÈáè‰ºòÂåñÂâç/ÂêéÂπ∂ËøõË°åÂØπÊØî„ÄÇ",
        "metrics_clarity_title": "ÊåáÊ†áÂà§ÂÆöËØ¥Êòé",
        "metrics_clarity_1": "Âú∫ÊôØÔºö‰∏ÄÊ¨°ÊúçÂä°Âô®Áä∂ÊÄÅÂø´ÁÖßÔºàGPU Êï∞/Âà©Áî®Áéá/ÊòæÂ≠ò/NUMA/NVLinkÔºâ„ÄÇ",
        "metrics_clarity_2": "Á§∫‰æãÂú∫ÊôØÔºöÈ¢ÑÁΩÆÁöÑÊºîÁ§∫/ÊµãËØïÊúçÂä°Âô®Áä∂ÊÄÅÊï∞ÊçÆ„ÄÇ",
        "metrics_clarity_3": "Êé®ËçêÔºö‰ªªÂä°Âú® GPU/NUMA ‰∏äÁöÑÊîæÁΩÆÂª∫ËÆÆ„ÄÇ",
        "metrics_clarity_4": "Êé®Ëçê‰∏ÄËá¥ÁéáÔºöÊúâÁì∂È¢àÂ∫îÊúâÊé®ËçêÔºåÊó†Áì∂È¢àÂ∫îÊó†Êé®ËçêÔºåÊâçÁÆó‰∏ÄËá¥„ÄÇ",
        "metrics_clarity_5": "Á§∫‰æãÔºöÊúâÁì∂È¢à+ÊúâÊé®Ëçê=‰∏ÄËá¥ÔºõÊó†Áì∂È¢à+Êó†Êé®Ëçê=‰∏ÄËá¥„ÄÇ",
        "metrics_phase": "Èò∂ÊÆµ",
        "metrics_phase_before": "‰ºòÂåñÂâç",
        "metrics_phase_after": "‰ºòÂåñÂêé",
        "metrics_iterations": "ÈáçÂ§çÊ¨°Êï∞",
        "metrics_run": "ÊâßË°åÊàêÂäüÊåáÊ†áÊµãÈáè",
        "metrics_recent": "ÊúÄËøëÊó•Âøó",
        "metrics_log_path": "Êó•ÂøóÊñá‰ª∂",
        "metrics_compare": "ÂâçÂêéÂØπÊØîÊëòË¶Å",
        "metrics_no_compare": "ÂâçÂêéËÆ∞ÂΩï‰∏çË∂≥ÔºåÊó†Ê≥ïÊØîËæÉ„ÄÇ",
        "metrics_report_download": "‰∏ãËΩΩÂâçÂêéÂØπÊØîÊä•Âëä(.md)",
        "metrics_report_title": "ÊàêÂäüÊåáÊ†áÂâçÂêéÂØπÊØîÊä•Âëä",
        "metrics_desc_response": "ÂìçÂ∫îÊó∂Èó¥ÔºöË∂ä‰ΩéË∂äÂ•Ω„ÄÇscore ÊòØËØÑÂàÜËÆ°ÁÆóÊó∂Èó¥Ôºåanalysis_pipeline ÊòØÁì∂È¢àÊ£ÄÊµã+Êé®ËçêÁîüÊàêÊó∂Èó¥„ÄÇ",
        "metrics_desc_test": "ÊµãËØïÈÄöËøáÁéáÔºöË∂äÈ´òË∂äÂ•Ω„ÄÇÈÄöËøáÊâßË°å `python -m unittest discover -s tests -p 'test_*.py'` ÂæóÂà∞„ÄÇ",
        "metrics_desc_test_formula": "ÂÖ¨ÂºèÔºöpassed_tests / total_tests",
        "metrics_desc_reco": "Êé®Ëçê‰∏ÄËá¥ÁéáÔºöË∂äÈ´òË∂äÂ•Ω„ÄÇË°®Á§∫Áì∂È¢àÊúâÊó†‰∏éÊé®ËçêÊòØÂê¶ÈúÄË¶ÅÁöÑ‰∏ÄËá¥Á®ãÂ∫¶„ÄÇ",
        "metrics_desc_reco_formula": "‰∏ÄËá¥Âà§ÂÆöÔºö(ÊúâÁì∂È¢à‰∏îÊúâÊé®Ëçê) Êàñ (Êó†Áì∂È¢à‰∏îÊó†Êé®Ëçê)",
        "metrics_desc_reco_count": "ÂÖ¨ÂºèÔºöconsistency_successes / attempted_scenarios",
        "metrics_desc_reco_detail": "ÁªÜÂàôÔºöÂ∞ùËØïÊï∞=Âú∫ÊôØÊï∞Èáè x ÈáçÂ§çÊ¨°Êï∞„ÄÇËØ•ÊåáÊ†áË°°Èáè‚ÄúÊòØÂê¶ËØ•Êé®Ëçê‚ÄùÁöÑÂà§Êñ≠‰∏ÄËá¥ÊÄßÔºå‰∏ç‰ª£Ë°®Êé®ËçêË¥®Èáè„ÄÇ",
        "metrics_term_scenario": "Âú∫ÊôØÔºö‰∏ÄÊ¨°ÂæÖÂàÜÊûêÁöÑÊúçÂä°Âô®Áä∂ÊÄÅÂø´ÁÖßÔºàGPU Êï∞„ÄÅÂà©Áî®Áéá„ÄÅÊòæÂ≠ò„ÄÅNUMA/NVLinkÔºâ„ÄÇ",
        "metrics_term_sample_scenario": "Á§∫‰æãÂú∫ÊôØÔºöÈ¢ÑÁΩÆÁöÑÊºîÁ§∫/ÊµãËØïÊúçÂä°Âô®Áä∂ÊÄÅÊï∞ÊçÆ„ÄÇ",
        "metrics_term_recommendation": "Êé®ËçêÔºö‰∏∫ÊèêÈ´òÊïàÁéáÁªôÂá∫‰ªªÂä°Âà∞ GPU/NUMA ÁöÑÊîæÁΩÆÂª∫ËÆÆÔºàÂ¶Ç NVLink Êú¨Âú∞ÊîæÁΩÆ„ÄÅMIG ÂàáÂàÜÔºâ„ÄÇ",
        "exec_settings_title": "ÊâßË°åËÆæÁΩÆ",
        "exec_show_host_ref": "ÂêåÊó∂ÊòæÁ§∫‰∏ªÊú∫ÂëΩ‰ª§ÔºàÂèÇËÄÉÔºâ",
        "exec_host_ref_title": "Host reference commands",
        "exec_validation_prefix": "ÊâßË°åËÆæÁΩÆÊ£ÄÊü•",
        "exec_validation_entry": "ËØ∑ËæìÂÖ•ÂÖ•Âè£ÂëΩ‰ª§„ÄÇ",
        "exec_validation_image": "Docker Ê®°Âºè‰∏ãÂøÖÈ°ªÂ°´ÂÜôÈïúÂÉè„ÄÇ",
        "exec_validation_cpu": "Manual CPU Set Ê®°Âºè‰∏ãÂøÖÈ°ªÂ°´ÂÜô CPU Set„ÄÇ",
        "exec_validation_env": "Env Vars Ê†ºÂºèÈîôËØØË°å: {lines}",
        "report_labels": {
            "scenario": "Âú∫ÊôØ",
            "efficiency": "ÊïàÁéáËØÑÂàÜ",
            "component_scores": "ÁªÑ‰ª∂ÂàÜÊï∞",
            "gpu_component": "GPU ÁªÑ‰ª∂ÂàÜ",
            "numa_component": "NUMA ÁªÑ‰ª∂ÂàÜ",
            "network_component": "ÁΩëÁªú I/O ÁªÑ‰ª∂ÂàÜ",
            "findings": "Áì∂È¢à/‰ΩéÊïàÂèëÁé∞",
            "analysis": "AI Áì∂È¢à/‰ΩéÊïàÂàÜÊûê",
            "placement": "ÊúÄ‰ºòÈÉ®ÁΩ≤Âª∫ËÆÆ",
            "expected": "È¢ÑÊúüÊîπÂñÑ",
        },
    },
}


def render_llm_loading(language_text: str, detail_text: str) -> str:
    return f"""
<style>
.infralens-loader {{
  border: 1px solid #e5e7eb; border-radius: 10px; padding: 10px 12px; margin: 8px 0 12px 0;
  background: linear-gradient(90deg, #f8fafc, #ffffff);
}}
.infralens-loader .line1 {{ font-weight: 600; margin-bottom: 4px; }}
.infralens-loader .line2 {{ color: #4b5563; font-size: 0.92rem; }}
.infralens-row {{ display: flex; align-items: center; gap: 10px; }}
.gear {{
  display: inline-block; font-size: 1.2rem; animation: spin 1.2s linear infinite;
}}
.car {{
  display: inline-block; font-size: 1.1rem; animation: drive 2.2s ease-in-out infinite;
}}
@keyframes spin {{ from {{ transform: rotate(0deg); }} to {{ transform: rotate(360deg); }} }}
@keyframes drive {{
  0% {{ transform: translateX(0px); }}
  50% {{ transform: translateX(10px); }}
  100% {{ transform: translateX(0px); }}
}}
</style>
<div class="infralens-loader">
  <div class="infralens-row"><span class="gear">‚öôÔ∏è</span><span class="car">üöó</span><span class="line1">{language_text}</span></div>
  <div class="line2">{detail_text}</div>
</div>
"""


st.set_page_config(page_title="InfraLens MVP", layout="wide")
SHOW_SUCCESS_METRICS = False  # NOTE: 'ÏÑ±Í≥µÏßÄÌëú Ï∏°Ï†ï' ÏÑπÏÖòÏùÄ Ï†ïÏùò ÌôïÏ†ï Ï†ÑÍπåÏßÄ Ï£ºÏÑù Ï≤òÎ¶¨(ÎπÑÎÖ∏Ï∂ú) ÏÉÅÌÉúÎ°ú Ïú†ÏßÄ.
st.markdown(
    """
<style>
[data-testid="stSidebar"] {
  width: 30vw;
  min-width: 30vw;
  max-width: 30vw;
}
[data-testid="stSidebar"] > div:first-child { width: 30vw; }
</style>
""",
    unsafe_allow_html=True,
)

lang_label_map = {"ÌïúÍµ≠Ïñ¥": "ko", "English": "en", "‰∏≠Êñá": "zh"}
with st.sidebar:
    lang_ui = st.selectbox("Language / Ïñ∏Ïñ¥ / ËØ≠Ë®Ä", list(lang_label_map.keys()), index=0)
lang = lang_label_map[lang_ui]
t = I18N[lang]

st.title(t["title"])
st.caption(t["caption"])

scenarios = sample_scenarios()
selected_name = ""
scenario = None

with st.sidebar:
    st.subheader(t["source"])
    data_source = st.radio("", [t["sample"], t["upload"]], horizontal=True, label_visibility="collapsed")
    st.caption(t.get("telemetry_word_help", ""))
    if data_source == t["sample"]:
        st.caption(t.get("sample_scenario_help", ""))
        selected_name = st.selectbox(t["scenario"], list(scenarios.keys()), help=t.get("scenario_help", ""))
        scenario = scenarios[selected_name]
    else:
        upload = st.file_uploader(t["upload_label"], type=["csv"])
        topo_upload = st.file_uploader(t["upload_topo_label"], type=["txt"], key="topo_upload")
        numa_upload = st.file_uploader(t["upload_numa_label"], type=["txt"], key="numa_upload")
        if upload is not None:
            try:
                topo_text = topo_upload.getvalue().decode("utf-8", errors="ignore") if topo_upload else None
                numa_text = numa_upload.getvalue().decode("utf-8", errors="ignore") if numa_upload else None
                scenario = parse_uploaded_telemetry(
                    upload.name,
                    upload.getvalue(),
                    topo_text=topo_text,
                    numactl_text=numa_text,
                )
                selected_name = scenario["name"]
                st.success(t["upload_ok"].format(n=len(scenario["gpus"]), name=upload.name))
                if topo_upload is not None:
                    st.caption(
                        t["upload_topo_ok"].format(
                            topo=topo_upload.name,
                            numa=numa_upload.name if numa_upload is not None else "N/A",
                        )
                    )
            except Exception as exc:
                st.error(t["upload_fail"].format(err=exc))
        else:
            st.info(t["upload_info"])

    st.subheader(t["llm_title"])
    st.caption(t["llm_help"])
    llm_enabled = st.toggle(t["llm_enable"], value=False)
    llm_provider_ui = st.selectbox(t["llm_provider"], ["OpenAI", "Claude", "Google"], index=0, disabled=not llm_enabled)
    llm_api_key_input = st.text_input(
        t["llm_api_key"].format(provider=llm_provider_ui),
        value="",
        type="password",
        disabled=not llm_enabled,
    )
    provider_map = {"OpenAI": "openai", "Claude": "anthropic", "Google": "google"}
    llm_provider = provider_map.get(llm_provider_ui, "openai")
    model_options, model_error = list_provider_models(llm_provider, llm_api_key_input if llm_enabled else None)
    if llm_enabled and llm_api_key_input.strip():
        if model_error is None:
            st.caption(t["llm_models_loaded"].format(provider=llm_provider_ui))
        elif model_error == "fetch_failed":
            st.caption(t["llm_models_fallback"])
    llm_model_input = st.selectbox(
        t["llm_model"],
        options=model_options,
        index=0,
        disabled=not llm_enabled,
    )

    st.subheader(t.get("exec_settings_title", "Execution Settings"))
    exec_env_ui = st.selectbox("Environment", ["Bare Metal", "Docker"], index=0)
    exec_entry = st.text_input("Entry Command", value="python train.py")
    exec_workdir = st.text_input("Workdir (Optional)", value="")
    exec_image = st.text_input("Docker Image", value="your-image:latest", disabled=exec_env_ui != "Docker")
    exec_prefix = st.text_input("Container Name Prefix", value="infralens-job")
    exec_args = st.text_input("Extra Args", value="")
    exec_envvars_raw = st.text_area("Env Vars (KEY=VALUE per line)", value="", height=90)
    exec_cpumode_ui = st.selectbox("CPU Set Mode", ["Auto", "Manual"], index=0)
    exec_cpumanual = st.text_input("Manual CPU Set", value="", disabled=exec_cpumode_ui != "Manual")
    exec_gpustyle_ui = st.selectbox("GPU Visibility Style", ["CUDA_VISIBLE_DEVICES", "--gpus device"], index=0)

def _normalize_cmd_text(cmd: str) -> str:
    # Some pipelines pass escaped newlines. Normalize before code rendering.
    return cmd.replace("\\n", "\n")


def _recommendation_rate(record: dict) -> float:
    if "recommendation_consistency" in record:
        return float(record["recommendation_consistency"].get("consistency_rate", 0.0))
    legacy = record.get("recommendation_generation", {})
    return float(legacy.get("success_rate", 0.0))


with st.expander(t["cmd_title"], expanded=False):
    st.caption(t["cmd_gpu"])
    st.code(
        "nvidia-smi --query-gpu=index,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits > nvidia_smi.csv",
        language="bash",
    )
    st.caption(t["cmd_topo"])
    st.code("nvidia-smi topo -m > nvidia_smi_topo_m.txt", language="bash")
    st.caption(t["cmd_numa"])
    st.code("numactl --hardware > numactl_hardware.txt", language="bash")

if SHOW_SUCCESS_METRICS:
    with st.expander(t.get("metrics_title", "Success Metrics"), expanded=False):
        st.caption(t.get("metrics_help", ""))
        st.markdown(f"**{t.get('metrics_clarity_title', 'How Metrics Are Measured')}**")
        st.markdown(
            "\n".join(
                [
                    f"- {t.get('metrics_clarity_1', '')}",
                    f"- {t.get('metrics_clarity_2', '')}",
                    f"- {t.get('metrics_clarity_3', '')}",
                    f"- {t.get('metrics_clarity_4', '')}",
                    f"- {t.get('metrics_clarity_5', '')}",
                ]
            )
        )
        st.info(
            "\n".join(
                [
                    f"- {t.get('metrics_term_scenario', '')}",
                    f"- {t.get('metrics_term_sample_scenario', '')}",
                    f"- {t.get('metrics_term_recommendation', '')}",
                ]
            )
        )
        st.markdown(
            "\n".join(
                [
                    f"- {t.get('metrics_desc_response', '')}",
                    f"- {t.get('metrics_desc_test', '')}",
                    f"  - {t.get('metrics_desc_test_formula', '')}",
                    f"- {t.get('metrics_desc_reco', '')}",
                    f"  - {t.get('metrics_desc_reco_formula', '')}",
                    f"  - {t.get('metrics_desc_reco_count', '')}",
                    f"  - {t.get('metrics_desc_reco_detail', '')}",
                ]
            )
        )
        metrics_phase_label = st.selectbox(
            t.get("metrics_phase", "Phase"),
            [t.get("metrics_phase_before", "Before Optimization"), t.get("metrics_phase_after", "After Optimization")],
            index=0,
            key="metrics_phase_select",
        )
        phase_map = {
            t.get("metrics_phase_before", "Before Optimization"): "before",
            t.get("metrics_phase_after", "After Optimization"): "after",
        }
        metrics_iterations = st.number_input(
            t.get("metrics_iterations", "Iterations"),
            min_value=1,
            max_value=20,
            value=3,
            step=1,
            key="metrics_iterations_input",
        )
        if st.button(t.get("metrics_run", "Run Success Metrics"), key="metrics_run_btn"):
            with st.spinner("collecting metrics..."):
                record, log_path = collect_success_metrics(
                    iterations=int(metrics_iterations),
                    out_path="logs/success_metrics.jsonl",
                    phase=phase_map.get(metrics_phase_label, ""),
                )
            st.success(f'{t.get("metrics_log_path", "Log file")}: {log_path}')
            st.json(record)

        recent = load_recent_metrics("logs/success_metrics.jsonl", limit=10)
        if recent:
            st.caption(t.get("metrics_recent", "Recent Metric Logs"))
            st.dataframe(pd.DataFrame(recent), use_container_width=True)

            before = [r for r in recent if r.get("phase") == "before"]
            after = [r for r in recent if r.get("phase") == "after"]
            if before and after:
                b = before[-1]
                a = after[-1]
                st.caption(t.get("metrics_compare", "Before/After Summary"))
                c1, c2, c3 = st.columns(3)
                c1.metric(
                    "Score Avg (s)",
                    value=f'{a["response_time_sec"]["score_avg"]:.6f}',
                    delta=round(a["response_time_sec"]["score_avg"] - b["response_time_sec"]["score_avg"], 6),
                )
                c2.metric(
                    "Test Pass Rate",
                    value=f'{a["test_pass_rate"]["pass_rate"]:.2%}',
                    delta=round(a["test_pass_rate"]["pass_rate"] - b["test_pass_rate"]["pass_rate"], 6),
                )
                c3.metric(
                    "Recommendation Consistency",
                    value=f'{_recommendation_rate(a):.2%}',
                    delta=round(
                        _recommendation_rate(a) - _recommendation_rate(b),
                        6,
                    ),
                )
                report_md = "\n".join(
                    [
                        f"# {t.get('metrics_report_title', 'Success Metrics Before/After Report')}",
                        "",
                        f"- Before timestamp: {b.get('timestamp_utc', '')}",
                        f"- After timestamp: {a.get('timestamp_utc', '')}",
                        "",
                        "## Response Time",
                        f"- score_avg: {b['response_time_sec']['score_avg']:.6f} -> {a['response_time_sec']['score_avg']:.6f}",
                        f"- score_p95: {b['response_time_sec']['score_p95']:.6f} -> {a['response_time_sec']['score_p95']:.6f}",
                        f"- analysis_pipeline_avg: {b['response_time_sec']['analysis_pipeline_avg']:.6f} -> {a['response_time_sec']['analysis_pipeline_avg']:.6f}",
                        f"- analysis_pipeline_p95: {b['response_time_sec']['analysis_pipeline_p95']:.6f} -> {a['response_time_sec']['analysis_pipeline_p95']:.6f}",
                        "",
                        "## Test Pass Rate",
                        f"- pass_rate: {b['test_pass_rate']['pass_rate']:.6f} -> {a['test_pass_rate']['pass_rate']:.6f}",
                        f"- passed/total: {b['test_pass_rate']['passed']}/{b['test_pass_rate']['total']} -> {a['test_pass_rate']['passed']}/{a['test_pass_rate']['total']}",
                        "",
                        "## Recommendation Generation",
                        f"- consistency_rate: {_recommendation_rate(b):.6f} -> {_recommendation_rate(a):.6f}",
                        f"- bottleneck_scenarios: {b.get('recommendation_consistency', {}).get('bottleneck_scenarios', 'N/A')} -> {a.get('recommendation_consistency', {}).get('bottleneck_scenarios', 'N/A')}",
                        f"- recommended_scenarios: {b.get('recommendation_consistency', {}).get('recommended_scenarios', 'N/A')} -> {a.get('recommendation_consistency', {}).get('recommended_scenarios', 'N/A')}",
                        f"- consistency_successes/attempts: {b.get('recommendation_consistency', {}).get('consistency_successes', 'N/A')}/{b.get('recommendation_consistency', {}).get('attempts', 'N/A')} -> {a.get('recommendation_consistency', {}).get('consistency_successes', 'N/A')}/{a.get('recommendation_consistency', {}).get('attempts', 'N/A')}",
                    ]
                )
                st.download_button(
                    label=t.get("metrics_report_download", "Download Before/After Report (.md)"),
                    data=report_md,
                    file_name="success_metrics_before_after_report.md",
                    mime="text/markdown",
                    key="metrics_report_download_btn",
                )
            else:
                st.info(t.get("metrics_no_compare", "Not enough before/after records yet."))

st.subheader(t["workload"])
st.caption(t.get("workload_help", ""))
if "workloads" not in st.session_state:
    st.session_state.workloads = [w.__dict__.copy() for w in default_workloads()]
if data_source == t["sample"]:
    prev_name = st.session_state.get("selected_scenario_name")
    if selected_name and prev_name != selected_name:
        st.session_state.workloads = [w.__dict__.copy() for w in workloads_for_scenario(selected_name)]
        st.session_state.selected_scenario_name = selected_name

workloads_df = pd.DataFrame(st.session_state.workloads)
edited_df = st.data_editor(
    workloads_df,
    num_rows="dynamic",
    use_container_width=True,
    column_config={
        "name": st.column_config.TextColumn("name"),
        "kind": st.column_config.SelectboxColumn("kind", options=["training", "inference"]),
        "gpu_demand": st.column_config.NumberColumn(
            "gpu_demand",
            min_value=1,
            max_value=max(1, len(scenario["gpus"])) if scenario else 8,
            step=1,
        ),
        "vram_gb": st.column_config.NumberColumn("vram_gb", min_value=1, max_value=160, step=1),
    },
)
st.session_state.workloads = edited_df.to_dict("records")
workloads = [Workload(**w) for w in st.session_state.workloads]

validation = validate_execution_settings(
    exec_env_ui=exec_env_ui,
    exec_entry=exec_entry,
    exec_image=exec_image,
    exec_cpumode_ui=exec_cpumode_ui,
    exec_cpumanual=exec_cpumanual,
    exec_envvars_raw=exec_envvars_raw,
)
parsed_env_vars = validation.env_vars
exec_validation_errors: list[str] = []
for err in validation.errors:
    if err.startswith("Entry command"):
        exec_validation_errors.append(t.get("exec_validation_entry", err))
    elif err.startswith("Docker image"):
        exec_validation_errors.append(t.get("exec_validation_image", err))
    elif err.startswith("Manual CPU set"):
        exec_validation_errors.append(t.get("exec_validation_cpu", err))
    elif err.startswith("Invalid env var line(s):"):
        lines = err.split(": ", 1)[1] if ": " in err else ""
        exec_validation_errors.append(t.get("exec_validation_env", err).format(lines=lines))
    else:
        exec_validation_errors.append(err)
if exec_validation_errors:
    st.warning(t.get("exec_validation_prefix", "Execution settings check") + ": " + " | ".join(exec_validation_errors))

analyze_clicked = st.button(t["analyze"], type="primary", disabled=(scenario is None or bool(exec_validation_errors)))
if analyze_clicked:
    profile = infer_workload_profile(workloads)
    score = calculate_efficiency_score(scenario, profile=profile)
    findings_raw = detect_bottlenecks(scenario, workloads, profile=profile)
    recommendation_raw = build_placement_recommendation(scenario, workloads, score.score, profile=profile)
    findings = localize_findings(findings_raw, lang)
    recommendation = localize_recommendation(recommendation_raw, lang)

    llm_api_key = llm_api_key_input.strip() if llm_enabled else None
    llm_model = llm_model_input.strip() if llm_enabled else None
    llm_loading_placeholder = st.empty()
    if llm_enabled:
        llm_loading_placeholder.markdown(
            render_llm_loading(t["llm_working"], t["llm_working_detail"]),
            unsafe_allow_html=True,
        )
    analysis_text, analysis_source = generate_bottleneck_analysis(
        findings,
        score.score,
        score.grade,
        language=lang,
        provider=llm_provider,
        api_key=llm_api_key or None,
        model=llm_model or None,
    )
    rec_text, rec_source = generate_recommendation_narrative(
        recommendation,
        language=lang,
        provider=llm_provider,
        api_key=llm_api_key or None,
        model=llm_model or None,
    )
    llm_loading_placeholder.empty()

    st.session_state.analysis_payload = {
        "scenario_name": selected_name,
        "scenario": scenario,
        "workloads": workloads,
        "score": score,
        "findings": findings,
        "analysis_text": analysis_text,
        "analysis_source": analysis_source,
        "recommendation": recommendation,
        "recommendation_raw": recommendation_raw,
        "rec_text": rec_text,
        "rec_source": rec_source,
    }

payload = st.session_state.get("analysis_payload")
if payload:
    score = payload["score"]
    findings = payload["findings"]
    analysis_text = payload["analysis_text"]
    analysis_source = payload["analysis_source"]
    recommendation = payload["recommendation"]
    recommendation_raw = payload["recommendation_raw"]
    rec_text = payload["rec_text"]
    rec_source = payload["rec_source"]
    analyzed_scenario = payload["scenario"]
    analyzed_workloads = payload["workloads"]
    analyzed_scenario_name = payload["scenario_name"]

    c1, c2, c3, c4 = st.columns(4)
    c1.metric(t["score"], f"{score.score}")
    c2.metric(t["grade"], score.grade)
    c3.metric(t["gpu_component"], f"{score.gpu_score:.2f}")
    c4.metric(t["numa_component"], f"{score.numa_score:.2f}")
    profile_label_key = f"profile_{score.profile}"
    if profile_label_key in t:
        st.caption(f'{t["profile"]}: {t[profile_label_key]}')

    st.subheader(t["telemetry"])
    st.caption(t.get("telemetry_help_ui", ""))
    st.dataframe(pd.DataFrame(analyzed_scenario["gpus"]), use_container_width=True)

    st.subheader(t["findings"])
    st.caption(t.get("findings_help", ""))
    for f in findings:
        tag = localize_severity(f.severity, lang)
        st.write(f"- **[{tag}] {f.category}**: {f.message}")

    st.subheader(t["analysis"])
    st.caption(t.get("analysis_help_ui", ""))
    st.caption(t["source_caption"].format(source=analysis_source))
    st.write(analysis_text)

    st.subheader(t["placement"])
    st.caption(t.get("placement_help", ""))
    for idx, item in enumerate(recommendation.items, 1):
        st.write(f"{idx}. **{item.workload}** - {item.action}")
    st.info(
        t["expected"].format(
            before=recommendation.expected_util_before,
            after=recommendation.expected_util_after,
            train=recommendation.expected_training_gain_pct,
            lat=recommendation.expected_latency_drop_pct,
        )
    )

    st.subheader(t["narrative"])
    st.caption(t.get("narrative_help", ""))
    st.caption(t["source_caption"].format(source=rec_source))
    st.write(rec_text)

    exec_cfg = ExecutionConfig(
        environment="docker" if exec_env_ui == "Docker" else "bare_metal",
        entry_command=exec_entry.strip() or "python train.py",
        workdir=exec_workdir.strip(),
        image_name=exec_image.strip() if exec_env_ui == "Docker" else "your-image:latest",
        container_prefix=exec_prefix.strip() or "infralens-job",
        extra_args=exec_args.strip(),
        env_vars=parsed_env_vars,
        cpu_set_mode="manual" if exec_cpumode_ui == "Manual" else "auto",
        manual_cpu_set=exec_cpumanual.strip(),
        gpu_visibility_style=(
            "cuda_visible_devices"
            if exec_gpustyle_ui == "CUDA_VISIBLE_DEVICES"
            else "docker_gpus_device"
        ),
    )
    cmd_templates = build_execution_templates(
        analyzed_scenario,
        analyzed_workloads,
        recommendation_raw,
        exec_cfg=exec_cfg,
    )
    if cmd_templates:
        st.subheader(t["exec_title"])
        st.caption(t["exec_desc"])
        if "show_host_ref_toggle" not in st.session_state:
            st.session_state.show_host_ref_toggle = False
        show_host_ref = st.toggle(
            t.get("exec_show_host_ref", "Also show host commands (reference)"),
            key="show_host_ref_toggle",
            disabled=exec_cfg.environment != "docker",
        )
        for tpl in cmd_templates:
            with st.expander(tpl.workload, expanded=False):
                if exec_cfg.environment == "docker":
                    st.caption(t["exec_docker"])
                    st.code(_normalize_cmd_text(tpl.docker_cmd), language="bash")
                    if show_host_ref:
                        with st.expander(t.get("exec_host_ref_title", "Host reference commands"), expanded=False):
                            st.caption(t["exec_numactl"])
                            st.code(_normalize_cmd_text(tpl.numactl_cmd), language="bash")
                            st.caption(t["exec_taskset"])
                            st.code(_normalize_cmd_text(tpl.taskset_cmd), language="bash")
                else:
                    st.caption(t["exec_numactl"])
                    st.code(_normalize_cmd_text(tpl.numactl_cmd), language="bash")
                    st.caption(t["exec_taskset"])
                    st.code(_normalize_cmd_text(tpl.taskset_cmd), language="bash")

    st.download_button(
        label=t["pdf"],
        data=build_pdf_report(
            scenario_name=analyzed_scenario_name,
            score=score,
            findings=findings,
            analysis_text=analysis_text,
            recommendation=recommendation,
            recommendation_text=rec_text,
            labels=t["report_labels"],
            expected_line=t["expected"].format(
                before=recommendation.expected_util_before,
                after=recommendation.expected_util_after,
                train=recommendation.expected_training_gain_pct,
                lat=recommendation.expected_latency_drop_pct,
            ),
        ),
        file_name="infralens_report.pdf",
        mime="application/pdf",
    )
else:
    st.write(t["idle"])
