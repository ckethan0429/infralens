from __future__ import annotations

import pandas as pd
import streamlit as st

from infralens.commands import ExecutionConfig, build_execution_templates
from infralens.data import Workload, default_workloads, sample_scenarios, workloads_for_scenario
from infralens.i18n import localize_findings, localize_recommendation, localize_severity
from infralens.llm import (
    generate_bottleneck_analysis,
    generate_recommendation_narrative,
    list_provider_models,
)
from infralens.parsers import parse_uploaded_telemetry
from infralens.report import build_pdf_report
from infralens.rules import build_placement_recommendation, detect_bottlenecks
from infralens.scoring import calculate_efficiency_score, infer_workload_profile


I18N = {
    "ko": {
        "title": "InfraLens - GPU Ïù∏ÌîÑÎùº ÏµúÏ†ÅÌôî AI ÏóêÏù¥Ï†ÑÌä∏",
        "caption": "MVP Îç∞Î™®: Ìö®Ïú® Ï†êÏàò + Î≥ëÎ™© Î∂ÑÏÑù + ÏµúÏ†Å Î∞∞Ïπò",
        "lang": "Ïñ∏Ïñ¥",
        "source": "ÌÖîÎ†àÎ©îÌä∏Î¶¨ ÏÜåÏä§",
        "sample": "ÏÉòÌîå Îç∞Ïù¥ÌÑ∞",
        "upload": "nvidia-smi ÏóÖÎ°úÎìú",
        "scenario": "ÏãúÎÇòÎ¶¨Ïò§",
        "upload_label": "ÌÖîÎ†àÎ©îÌä∏Î¶¨ ÌååÏùº ÏóÖÎ°úÎìú (.csv)",
        "upload_topo_label": "ÏÑ†ÌÉù: nvidia-smi topo ÌååÏùº ÏóÖÎ°úÎìú (.txt)",
        "upload_numa_label": "ÏÑ†ÌÉù: numactl --hardware ÌååÏùº ÏóÖÎ°úÎìú (.txt)",
        "upload_info": "Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ ÌÖîÎ†àÎ©îÌä∏Î¶¨ ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî.",
        "upload_ok": "GPU {n}Í∞úÎ•º {name}ÏóêÏÑú Î°úÎìúÌñàÏäµÎãàÎã§.",
        "upload_topo_ok": "ÌÜ†Ìè¥Î°úÏßÄ Ïò§Î≤ÑÎùºÏù¥Îìú Ï†ÅÏö©Îê® (topo: {topo}, numactl: {numa})",
        "upload_fail": "ÏóÖÎ°úÎìú ÌååÏã± Ïã§Ìå®: {err}",
        "workload": "ÏõåÌÅ¨Î°úÎìú ÏÑ§Ï†ï",
        "analyze": "Î∂ÑÏÑù ÏãúÏûë",
        "score": "Ìö®Ïú® Ï†êÏàò",
        "grade": "Îì±Í∏â",
        "gpu_component": "GPU Íµ¨ÏÑ± Ï†êÏàò",
        "numa_component": "NUMA Íµ¨ÏÑ± Ï†êÏàò",
        "telemetry": "ÌÖîÎ†àÎ©îÌä∏Î¶¨ Ïä§ÎÉÖÏÉ∑",
        "findings": "Î≥ëÎ™© Î∂ÑÏÑù Í≤∞Í≥º",
        "analysis": "AI Î≥ëÎ™© Î∂ÑÏÑù",
        "placement": "ÏµúÏ†Å Î∞∞Ïπò Ï†úÏïà",
        "expected": "ÏòàÏÉÅ Í∞úÏÑ†: ÌôúÏö©Î•† {before}% -> {after}%, ÌïôÏäµ +{train}%, Ï∂îÎ°† Î†àÏù¥ÌÑ¥Ïãú -{lat}%",
        "narrative": "Ï∂îÏ≤ú ÏÑ§Î™Ö",
        "exec_title": "Ïã§Ìñâ Í∞ÄÎä•Ìïú Î™ÖÎ†πÏñ¥ ÌÖúÌîåÎ¶ø",
        "exec_desc": "Ï∂îÏ≤ú Í≤∞Í≥º Í∏∞Î∞ò ÌÖúÌîåÎ¶øÏûÖÎãàÎã§. ÌôòÍ≤ΩÏóê ÎßûÍ≤å Ïù¥ÎØ∏ÏßÄ/Ïä§ÌÅ¨Î¶ΩÌä∏ Í≤ΩÎ°úÎ•º ÏàòÏ†ïÌïòÏÑ∏Ïöî.",
        "exec_numactl": "numactl",
        "exec_taskset": "taskset",
        "exec_docker": "docker",
        "pdf": "PDF Î¶¨Ìè¨Ìä∏ Îã§Ïö¥Î°úÎìú",
        "idle": "ÏãúÎÇòÎ¶¨Ïò§Î•º ÏÑ†ÌÉùÌïòÍ≥† Î∂ÑÏÑù ÏãúÏûëÏùÑ ÎàåÎü¨ ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ïã§ÌñâÌïòÏÑ∏Ïöî.",
        "source_caption": "ÏÉùÏÑ± ÏÜåÏä§: {source}",
        "profile": "ÌîÑÎ°úÌïÑ",
        "profile_default": "Í∏∞Î≥∏",
        "profile_training": "ÌïôÏäµ Ï§ëÏã¨",
        "profile_inference": "Ï∂îÎ°† Ï§ëÏã¨",
        "cmd_title": "Ïã§Ìñâ Î™ÖÎ†π (Î≥µÏÇ¨Ìï¥ÏÑú ÏÇ¨Ïö©)",
        "cmd_gpu": "1) GPU ÌÖîÎ†àÎ©îÌä∏Î¶¨ CSV",
        "cmd_topo": "2) GPU ÌÜ†Ìè¥Î°úÏßÄ ÌÖçÏä§Ìä∏",
        "cmd_numa": "3) NUMA ÌÜ†Ìè¥Î°úÏßÄ ÌÖçÏä§Ìä∏",
        "llm_title": "LLM API ÏÑ§Ï†ï",
        "llm_enable": "UIÏóêÏÑú LLM API ÏÇ¨Ïö©",
        "llm_provider": "Provider",
        "llm_api_key": "API Key ({provider})",
        "llm_model": "Model",
        "llm_help": "API KeyÎ•º ÏûÖÎ†•ÌïòÏßÄ ÏïäÏúºÎ©¥ Î£∞ Í∏∞Î∞ò fallbackÏúºÎ°ú ÎèôÏûëÌï©ÎãàÎã§.",
        "llm_models_loaded": "{provider} Î™®Îç∏ Î™©Î°ùÏùÑ Î∂àÎü¨ÏôîÏäµÎãàÎã§.",
        "llm_models_fallback": "Î™®Îç∏ Î™©Î°ù Ï°∞ÌöåÏóê Ïã§Ìå®Ìï¥ Í∏∞Î≥∏ Ï∂îÏ≤ú Î™®Îç∏ÏùÑ ÌëúÏãúÌï©ÎãàÎã§.",
        "llm_working": "LLM Î∂ÑÏÑù/Ï∂îÏ≤ú ÏÉùÏÑ± Ï§ëÏûÖÎãàÎã§...",
        "llm_working_detail": "Ïû†ÏãúÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî. Î≥ëÎ™© Î∂ÑÏÑùÍ≥º Î∞∞Ïπò ÏÑ§Î™ÖÏùÑ ÏÉùÏÑ±ÌïòÍ≥† ÏûàÏäµÎãàÎã§.",
        "report_labels": {
            "scenario": "ÏãúÎÇòÎ¶¨Ïò§",
            "efficiency": "Ìö®Ïú® Ï†êÏàò",
            "component_scores": "Íµ¨ÏÑ± Ï†êÏàò",
            "gpu_component": "GPU Íµ¨ÏÑ± Ï†êÏàò",
            "numa_component": "NUMA Íµ¨ÏÑ± Ï†êÏàò",
            "network_component": "ÎÑ§Ìä∏ÏõåÌÅ¨ I/O Ï†êÏàò",
            "findings": "Î≥ëÎ™© Î∂ÑÏÑù Í≤∞Í≥º",
            "analysis": "AI Î≥ëÎ™© Î∂ÑÏÑù",
            "placement": "ÏµúÏ†Å Î∞∞Ïπò Ï†úÏïà",
            "expected": "ÏòàÏÉÅ Í∞úÏÑ†",
        },
    },
    "en": {
        "title": "InfraLens - GPU Infrastructure Optimization AI Agent",
        "caption": "MVP Demo: Resource Score + Bottleneck Analysis + Optimal Placement",
        "lang": "Language",
        "source": "Telemetry Source",
        "sample": "Sample Data",
        "upload": "Upload nvidia-smi",
        "scenario": "Scenario",
        "upload_label": "Upload telemetry file (.csv)",
        "upload_topo_label": "Optional: Upload nvidia-smi topology file (.txt)",
        "upload_numa_label": "Optional: Upload numactl --hardware file (.txt)",
        "upload_info": "Upload a telemetry file to run analysis.",
        "upload_ok": "Loaded {n} GPUs from {name}.",
        "upload_topo_ok": "Applied topology overrides (topo: {topo}, numactl: {numa})",
        "upload_fail": "Failed to parse upload: {err}",
        "workload": "Workload Configuration",
        "analyze": "Analyze",
        "score": "Efficiency Score",
        "grade": "Grade",
        "gpu_component": "GPU Component",
        "numa_component": "NUMA Component",
        "telemetry": "Telemetry Snapshot",
        "findings": "Bottleneck Findings",
        "analysis": "AI Bottleneck Analysis",
        "placement": "Optimal Placement Recommendation",
        "expected": "Expected improvement: Utilization {before}% -> {after}%, Training +{train}%, Inference Latency -{lat}%",
        "narrative": "Recommendation Narrative",
        "exec_title": "Executable Command Templates",
        "exec_desc": "Templates are generated from recommendations. Adjust image/script paths to your environment.",
        "exec_numactl": "numactl",
        "exec_taskset": "taskset",
        "exec_docker": "docker",
        "pdf": "Download PDF Report",
        "idle": "Select a scenario and click Analyze to run the MVP pipeline.",
        "source_caption": "Source: {source}",
        "profile": "Profile",
        "profile_default": "Default",
        "profile_training": "Training-heavy",
        "profile_inference": "Inference-heavy",
        "cmd_title": "Collection Commands (Copy)",
        "cmd_gpu": "1) GPU telemetry CSV",
        "cmd_topo": "2) GPU topology text",
        "cmd_numa": "3) NUMA topology text",
        "llm_title": "LLM API Settings",
        "llm_enable": "Use LLM API from UI",
        "llm_provider": "Provider",
        "llm_api_key": "API Key ({provider})",
        "llm_model": "Model",
        "llm_help": "If API key is empty, rule-based fallback will be used.",
        "llm_models_loaded": "Loaded model list from {provider}.",
        "llm_models_fallback": "Could not fetch model list, showing recommended defaults.",
        "llm_working": "Generating LLM analysis and recommendations...",
        "llm_working_detail": "Please wait while bottleneck analysis and placement narratives are being prepared.",
        "report_labels": {
            "scenario": "Scenario",
            "efficiency": "Efficiency Score",
            "component_scores": "Component Scores",
            "gpu_component": "GPU Component",
            "numa_component": "NUMA Component",
            "network_component": "Network I/O Component",
            "findings": "Bottleneck Findings",
            "analysis": "AI Bottleneck Analysis",
            "placement": "Optimal Placement Recommendation",
            "expected": "Expected Improvement",
        },
    },
    "zh": {
        "title": "InfraLens - GPU Âü∫Á°ÄËÆæÊñΩ‰ºòÂåñ AI Agent",
        "caption": "MVP ÊºîÁ§∫ÔºöÊïàÁéáËØÑÂàÜ + Áì∂È¢àÂàÜÊûê + ÊúÄ‰ºòÈÉ®ÁΩ≤",
        "lang": "ËØ≠Ë®Ä",
        "source": "ÈÅ•ÊµãÊï∞ÊçÆÊù•Ê∫ê",
        "sample": "Á§∫‰æãÊï∞ÊçÆ",
        "upload": "‰∏ä‰º† nvidia-smi",
        "scenario": "Âú∫ÊôØ",
        "upload_label": "‰∏ä‰º†ÈÅ•ÊµãÊñá‰ª∂Ôºà.csvÔºâ",
        "upload_topo_label": "ÂèØÈÄâÔºö‰∏ä‰º† nvidia-smi ÊãìÊâëÊñá‰ª∂Ôºà.txtÔºâ",
        "upload_numa_label": "ÂèØÈÄâÔºö‰∏ä‰º† numactl --hardware Êñá‰ª∂Ôºà.txtÔºâ",
        "upload_info": "ËØ∑‰∏ä‰º†ÈÅ•ÊµãÊñá‰ª∂ÂêéÂÜçÊâßË°åÂàÜÊûê„ÄÇ",
        "upload_ok": "Â∑≤‰ªé {name} Âä†ËΩΩ {n} Âº† GPU„ÄÇ",
        "upload_topo_ok": "Â∑≤Â∫îÁî®ÊãìÊâëË¶ÜÁõñÔºàtopo: {topo}, numactl: {numa}Ôºâ",
        "upload_fail": "Ëß£Êûê‰∏ä‰º†Êñá‰ª∂Â§±Ë¥•: {err}",
        "workload": "Â∑•‰ΩúË¥üËΩΩÈÖçÁΩÆ",
        "analyze": "ÂºÄÂßãÂàÜÊûê",
        "score": "ÊïàÁéáËØÑÂàÜ",
        "grade": "Á≠âÁ∫ß",
        "gpu_component": "GPU ÁªÑ‰ª∂ÂàÜ",
        "numa_component": "NUMA ÁªÑ‰ª∂ÂàÜ",
        "telemetry": "ÈÅ•ÊµãÂø´ÁÖß",
        "findings": "Áì∂È¢àÂèëÁé∞",
        "analysis": "AI Áì∂È¢àÂàÜÊûê",
        "placement": "ÊúÄ‰ºòÈÉ®ÁΩ≤Âª∫ËÆÆ",
        "expected": "È¢ÑÊúüÊîπÂñÑÔºöÂà©Áî®Áéá {before}% -> {after}%ÔºåËÆ≠ÁªÉ +{train}%ÔºåÊé®ÁêÜÊó∂Âª∂ -{lat}%",
        "narrative": "Âª∫ËÆÆËØ¥Êòé",
        "exec_title": "ÂèØÊâßË°åÂëΩ‰ª§Ê®°Êùø",
        "exec_desc": "Âü∫‰∫éÊé®ËçêÁªìÊûúÁîüÊàêÊ®°Êùø„ÄÇËØ∑ÊåâÁéØÂ¢É‰øÆÊîπÈïúÂÉèÂíåËÑöÊú¨Ë∑ØÂæÑ„ÄÇ",
        "exec_numactl": "numactl",
        "exec_taskset": "taskset",
        "exec_docker": "docker",
        "pdf": "‰∏ãËΩΩ PDF Êä•Âëä",
        "idle": "ËØ∑ÈÄâÊã©Âú∫ÊôØÂπ∂ÁÇπÂáªÂºÄÂßãÂàÜÊûê„ÄÇ",
        "source_caption": "Êù•Ê∫ê: {source}",
        "profile": "ÂàÜÊûêÊ°£‰Ωç",
        "profile_default": "ÈªòËÆ§",
        "profile_training": "ËÆ≠ÁªÉ‰ºòÂÖà",
        "profile_inference": "Êé®ÁêÜ‰ºòÂÖà",
        "cmd_title": "ÈááÈõÜÂëΩ‰ª§ÔºàÂèØÁõ¥Êé•Â§çÂà∂Ôºâ",
        "cmd_gpu": "1) GPU ÈÅ•Êµã CSV",
        "cmd_topo": "2) GPU ÊãìÊâëÊñáÊú¨",
        "cmd_numa": "3) NUMA ÊãìÊâëÊñáÊú¨",
        "llm_title": "LLM API ËÆæÁΩÆ",
        "llm_enable": "Âú® UI ‰∏≠‰ΩøÁî® LLM API",
        "llm_provider": "‰æõÂ∫îÂïÜ",
        "llm_api_key": "API KeyÔºà{provider}Ôºâ",
        "llm_model": "Ê®°Âûã",
        "llm_help": "Êú™Â°´ÂÜô API Key Êó∂Â∞Ü‰ΩøÁî®ËßÑÂàô fallback„ÄÇ",
        "llm_models_loaded": "Â∑≤‰ªé {provider} Âä†ËΩΩÊ®°ÂûãÂàóË°®„ÄÇ",
        "llm_models_fallback": "Ê®°ÂûãÂàóË°®Ëé∑ÂèñÂ§±Ë¥•ÔºåÂ∑≤ÊòæÁ§∫Êé®ËçêÈªòËÆ§Ê®°Âûã„ÄÇ",
        "llm_working": "Ê≠£Âú®ÁîüÊàê LLM ÂàÜÊûê‰∏éÂª∫ËÆÆ...",
        "llm_working_detail": "ËØ∑Á®çÂÄôÔºåÁ≥ªÁªüÊ≠£Âú®ÁîüÊàêÁì∂È¢àÂàÜÊûêÂíåÈÉ®ÁΩ≤Âª∫ËÆÆËØ¥Êòé„ÄÇ",
        "report_labels": {
            "scenario": "Âú∫ÊôØ",
            "efficiency": "ÊïàÁéáËØÑÂàÜ",
            "component_scores": "ÁªÑ‰ª∂ÂàÜÊï∞",
            "gpu_component": "GPU ÁªÑ‰ª∂ÂàÜ",
            "numa_component": "NUMA ÁªÑ‰ª∂ÂàÜ",
            "network_component": "ÁΩëÁªú I/O ÁªÑ‰ª∂ÂàÜ",
            "findings": "Áì∂È¢àÂèëÁé∞",
            "analysis": "AI Áì∂È¢àÂàÜÊûê",
            "placement": "ÊúÄ‰ºòÈÉ®ÁΩ≤Âª∫ËÆÆ",
            "expected": "È¢ÑÊúüÊîπÂñÑ",
        },
    },
}


def render_llm_loading(language_text: str, detail_text: str) -> str:
    return f"""
<style>
.infralens-loader {{
  border: 1px solid #e5e7eb; border-radius: 10px; padding: 10px 12px; margin: 8px 0 12px 0;
  background: linear-gradient(90deg, #f8fafc, #ffffff);
}}
.infralens-loader .line1 {{ font-weight: 600; margin-bottom: 4px; }}
.infralens-loader .line2 {{ color: #4b5563; font-size: 0.92rem; }}
.infralens-row {{ display: flex; align-items: center; gap: 10px; }}
.gear {{
  display: inline-block; font-size: 1.2rem; animation: spin 1.2s linear infinite;
}}
.car {{
  display: inline-block; font-size: 1.1rem; animation: drive 2.2s ease-in-out infinite;
}}
@keyframes spin {{ from {{ transform: rotate(0deg); }} to {{ transform: rotate(360deg); }} }}
@keyframes drive {{
  0% {{ transform: translateX(0px); }}
  50% {{ transform: translateX(10px); }}
  100% {{ transform: translateX(0px); }}
}}
</style>
<div class="infralens-loader">
  <div class="infralens-row"><span class="gear">‚öôÔ∏è</span><span class="car">üöó</span><span class="line1">{language_text}</span></div>
  <div class="line2">{detail_text}</div>
</div>
"""


st.set_page_config(page_title="InfraLens MVP", layout="wide")
st.markdown(
    """
<style>
[data-testid="stSidebar"] {
  width: 30vw;
  min-width: 30vw;
  max-width: 30vw;
}
[data-testid="stSidebar"] > div:first-child { width: 30vw; }
</style>
""",
    unsafe_allow_html=True,
)

lang_label_map = {"ÌïúÍµ≠Ïñ¥": "ko", "English": "en", "‰∏≠Êñá": "zh"}
with st.sidebar:
    lang_ui = st.selectbox("Language / Ïñ∏Ïñ¥ / ËØ≠Ë®Ä", list(lang_label_map.keys()), index=0)
lang = lang_label_map[lang_ui]
t = I18N[lang]

st.title(t["title"])
st.caption(t["caption"])

scenarios = sample_scenarios()
selected_name = ""
scenario = None

with st.sidebar:
    data_source = st.radio(t["source"], [t["sample"], t["upload"]], horizontal=True)
    if data_source == t["sample"]:
        selected_name = st.selectbox(t["scenario"], list(scenarios.keys()))
        scenario = scenarios[selected_name]
    else:
        upload = st.file_uploader(t["upload_label"], type=["csv"])
        topo_upload = st.file_uploader(t["upload_topo_label"], type=["txt"], key="topo_upload")
        numa_upload = st.file_uploader(t["upload_numa_label"], type=["txt"], key="numa_upload")
        if upload is not None:
            try:
                topo_text = topo_upload.getvalue().decode("utf-8", errors="ignore") if topo_upload else None
                numa_text = numa_upload.getvalue().decode("utf-8", errors="ignore") if numa_upload else None
                scenario = parse_uploaded_telemetry(
                    upload.name,
                    upload.getvalue(),
                    topo_text=topo_text,
                    numactl_text=numa_text,
                )
                selected_name = scenario["name"]
                st.success(t["upload_ok"].format(n=len(scenario["gpus"]), name=upload.name))
                if topo_upload is not None:
                    st.caption(
                        t["upload_topo_ok"].format(
                            topo=topo_upload.name,
                            numa=numa_upload.name if numa_upload is not None else "N/A",
                        )
                    )
            except Exception as exc:
                st.error(t["upload_fail"].format(err=exc))
        else:
            st.info(t["upload_info"])

    st.subheader(t["llm_title"])
    st.caption(t["llm_help"])
    llm_enabled = st.toggle(t["llm_enable"], value=False)
    llm_provider_ui = st.selectbox(t["llm_provider"], ["OpenAI", "Claude", "Google"], index=0, disabled=not llm_enabled)
    llm_api_key_input = st.text_input(
        t["llm_api_key"].format(provider=llm_provider_ui),
        value="",
        type="password",
        disabled=not llm_enabled,
    )
    provider_map = {"OpenAI": "openai", "Claude": "anthropic", "Google": "google"}
    llm_provider = provider_map.get(llm_provider_ui, "openai")
    model_options, model_error = list_provider_models(llm_provider, llm_api_key_input if llm_enabled else None)
    if llm_enabled and llm_api_key_input.strip():
        if model_error is None:
            st.caption(t["llm_models_loaded"].format(provider=llm_provider_ui))
        elif model_error == "fetch_failed":
            st.caption(t["llm_models_fallback"])
    llm_model_input = st.selectbox(
        t["llm_model"],
        options=model_options,
        index=0,
        disabled=not llm_enabled,
    )

    st.subheader("Execution Settings")
    exec_env_ui = st.selectbox("Environment", ["Bare Metal", "Docker"], index=0)
    exec_entry = st.text_input("Entry Command", value="python train.py")
    exec_workdir = st.text_input("Workdir (Optional)", value="")
    exec_image = st.text_input("Docker Image", value="your-image:latest", disabled=exec_env_ui != "Docker")
    exec_prefix = st.text_input("Container Name Prefix", value="infralens-job")
    exec_args = st.text_input("Extra Args", value="")
    exec_envvars_raw = st.text_area("Env Vars (KEY=VALUE per line)", value="", height=90)
    exec_cpumode_ui = st.selectbox("CPU Set Mode", ["Auto", "Manual"], index=0)
    exec_cpumanual = st.text_input("Manual CPU Set", value="", disabled=exec_cpumode_ui != "Manual")
    exec_gpustyle_ui = st.selectbox("GPU Visibility Style", ["CUDA_VISIBLE_DEVICES", "--gpus device"], index=0)


def _parse_env_vars(raw: str) -> tuple[dict[str, str], list[int]]:
    out: dict[str, str] = {}
    invalid_lines: list[int] = []
    for idx, line in enumerate(raw.splitlines(), 1):
        token = line.strip()
        if not token:
            continue
        if "=" not in token:
            invalid_lines.append(idx)
            continue
        k, v = token.split("=", 1)
        key = k.strip()
        if key:
            out[key] = v.strip()
        else:
            invalid_lines.append(idx)
    return out, invalid_lines


with st.expander(t["cmd_title"], expanded=False):
    st.caption(t["cmd_gpu"])
    st.code(
        "nvidia-smi --query-gpu=index,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits > nvidia_smi.csv",
        language="bash",
    )
    st.caption(t["cmd_topo"])
    st.code("nvidia-smi topo -m > nvidia_smi_topo_m.txt", language="bash")
    st.caption(t["cmd_numa"])
    st.code("numactl --hardware > numactl_hardware.txt", language="bash")

st.subheader(t["workload"])
if "workloads" not in st.session_state:
    st.session_state.workloads = [w.__dict__.copy() for w in default_workloads()]
if data_source == t["sample"]:
    prev_name = st.session_state.get("selected_scenario_name")
    if selected_name and prev_name != selected_name:
        st.session_state.workloads = [w.__dict__.copy() for w in workloads_for_scenario(selected_name)]
        st.session_state.selected_scenario_name = selected_name

workloads_df = pd.DataFrame(st.session_state.workloads)
edited_df = st.data_editor(
    workloads_df,
    num_rows="dynamic",
    use_container_width=True,
    column_config={
        "name": st.column_config.TextColumn("name"),
        "kind": st.column_config.SelectboxColumn("kind", options=["training", "inference"]),
        "gpu_demand": st.column_config.NumberColumn(
            "gpu_demand",
            min_value=1,
            max_value=max(1, len(scenario["gpus"])) if scenario else 8,
            step=1,
        ),
        "vram_gb": st.column_config.NumberColumn("vram_gb", min_value=1, max_value=160, step=1),
    },
)
st.session_state.workloads = edited_df.to_dict("records")
workloads = [Workload(**w) for w in st.session_state.workloads]

parsed_env_vars, invalid_env_lines = _parse_env_vars(exec_envvars_raw)
exec_validation_errors: list[str] = []
if not exec_entry.strip():
    exec_validation_errors.append("Entry command is required.")
if exec_env_ui == "Docker" and not exec_image.strip():
    exec_validation_errors.append("Docker image is required in Docker mode.")
if exec_cpumode_ui == "Manual" and not exec_cpumanual.strip():
    exec_validation_errors.append("Manual CPU set is required in Manual mode.")
if invalid_env_lines:
    exec_validation_errors.append(f"Invalid env var line(s): {','.join(str(i) for i in invalid_env_lines)}")
if exec_validation_errors:
    st.warning("Execution settings check: " + " | ".join(exec_validation_errors))

analyze_clicked = st.button(t["analyze"], type="primary", disabled=(scenario is None or bool(exec_validation_errors)))
if analyze_clicked:
    profile = infer_workload_profile(workloads)
    score = calculate_efficiency_score(scenario, profile=profile)
    findings_raw = detect_bottlenecks(scenario, workloads, profile=profile)
    recommendation_raw = build_placement_recommendation(scenario, workloads, score.score, profile=profile)
    findings = localize_findings(findings_raw, lang)
    recommendation = localize_recommendation(recommendation_raw, lang)

    llm_api_key = llm_api_key_input.strip() if llm_enabled else None
    llm_model = llm_model_input.strip() if llm_enabled else None
    llm_loading_placeholder = st.empty()
    if llm_enabled:
        llm_loading_placeholder.markdown(
            render_llm_loading(t["llm_working"], t["llm_working_detail"]),
            unsafe_allow_html=True,
        )
    analysis_text, analysis_source = generate_bottleneck_analysis(
        findings,
        score.score,
        score.grade,
        language=lang,
        provider=llm_provider,
        api_key=llm_api_key or None,
        model=llm_model or None,
    )
    rec_text, rec_source = generate_recommendation_narrative(
        recommendation,
        language=lang,
        provider=llm_provider,
        api_key=llm_api_key or None,
        model=llm_model or None,
    )
    llm_loading_placeholder.empty()

    st.session_state.analysis_payload = {
        "scenario_name": selected_name,
        "scenario": scenario,
        "workloads": workloads,
        "score": score,
        "findings": findings,
        "analysis_text": analysis_text,
        "analysis_source": analysis_source,
        "recommendation": recommendation,
        "recommendation_raw": recommendation_raw,
        "rec_text": rec_text,
        "rec_source": rec_source,
    }

payload = st.session_state.get("analysis_payload")
if payload:
    score = payload["score"]
    findings = payload["findings"]
    analysis_text = payload["analysis_text"]
    analysis_source = payload["analysis_source"]
    recommendation = payload["recommendation"]
    recommendation_raw = payload["recommendation_raw"]
    rec_text = payload["rec_text"]
    rec_source = payload["rec_source"]
    analyzed_scenario = payload["scenario"]
    analyzed_workloads = payload["workloads"]
    analyzed_scenario_name = payload["scenario_name"]

    c1, c2, c3, c4 = st.columns(4)
    c1.metric(t["score"], f"{score.score}")
    c2.metric(t["grade"], score.grade)
    c3.metric(t["gpu_component"], f"{score.gpu_score:.2f}")
    c4.metric(t["numa_component"], f"{score.numa_score:.2f}")
    profile_label_key = f"profile_{score.profile}"
    if profile_label_key in t:
        st.caption(f'{t["profile"]}: {t[profile_label_key]}')

    st.subheader(t["telemetry"])
    st.dataframe(pd.DataFrame(analyzed_scenario["gpus"]), use_container_width=True)

    st.subheader(t["findings"])
    for f in findings:
        tag = localize_severity(f.severity, lang)
        st.write(f"- **[{tag}] {f.category}**: {f.message}")

    st.subheader(t["analysis"])
    st.caption(t["source_caption"].format(source=analysis_source))
    st.write(analysis_text)

    st.subheader(t["placement"])
    for idx, item in enumerate(recommendation.items, 1):
        st.write(f"{idx}. **{item.workload}** - {item.action}")
    st.info(
        t["expected"].format(
            before=recommendation.expected_util_before,
            after=recommendation.expected_util_after,
            train=recommendation.expected_training_gain_pct,
            lat=recommendation.expected_latency_drop_pct,
        )
    )

    st.subheader(t["narrative"])
    st.caption(t["source_caption"].format(source=rec_source))
    st.write(rec_text)

    exec_cfg = ExecutionConfig(
        environment="docker" if exec_env_ui == "Docker" else "bare_metal",
        entry_command=exec_entry.strip() or "python train.py",
        workdir=exec_workdir.strip(),
        image_name=exec_image.strip() if exec_env_ui == "Docker" else "your-image:latest",
        container_prefix=exec_prefix.strip() or "infralens-job",
        extra_args=exec_args.strip(),
        env_vars=parsed_env_vars,
        cpu_set_mode="manual" if exec_cpumode_ui == "Manual" else "auto",
        manual_cpu_set=exec_cpumanual.strip(),
        gpu_visibility_style=(
            "cuda_visible_devices"
            if exec_gpustyle_ui == "CUDA_VISIBLE_DEVICES"
            else "docker_gpus_device"
        ),
    )
    cmd_templates = build_execution_templates(
        analyzed_scenario,
        analyzed_workloads,
        recommendation_raw,
        exec_cfg=exec_cfg,
    )
    if cmd_templates:
        st.subheader(t["exec_title"])
        st.caption(t["exec_desc"])
        show_host_ref = False
        if exec_cfg.environment == "docker":
            show_host_ref = st.toggle("Also show host commands", value=False, key="show_host_ref_toggle")
        for tpl in cmd_templates:
            with st.expander(tpl.workload, expanded=False):
                if exec_cfg.environment == "docker":
                    st.caption(t["exec_docker"])
                    st.code(tpl.docker_cmd, language="bash")
                    if show_host_ref:
                        st.caption(t["exec_numactl"])
                        st.code(tpl.numactl_cmd, language="bash")
                        st.caption(t["exec_taskset"])
                        st.code(tpl.taskset_cmd, language="bash")
                else:
                    st.caption(t["exec_numactl"])
                    st.code(tpl.numactl_cmd, language="bash")
                    st.caption(t["exec_taskset"])
                    st.code(tpl.taskset_cmd, language="bash")

    st.download_button(
        label=t["pdf"],
        data=build_pdf_report(
            scenario_name=analyzed_scenario_name,
            score=score,
            findings=findings,
            analysis_text=analysis_text,
            recommendation=recommendation,
            recommendation_text=rec_text,
            labels=t["report_labels"],
            expected_line=t["expected"].format(
                before=recommendation.expected_util_before,
                after=recommendation.expected_util_after,
                train=recommendation.expected_training_gain_pct,
                lat=recommendation.expected_latency_drop_pct,
            ),
        ),
        file_name="infralens_report.pdf",
        mime="application/pdf",
    )
else:
    st.write(t["idle"])
